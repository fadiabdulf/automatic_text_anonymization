{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "sys.path.insert(1, '..')\n",
    "from classes import DataLoader, MyIterator, Chunker, WordEmbedding, Evaluator, MyBertEmbedding\n",
    "from classes.utils import load_presidio\n",
    "from tqdm.auto import tqdm, trange\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = '../data/wiki/'\n",
    "out_dir = '../data/wiki/'\n",
    "loader = DataLoader(in_dir, out_dir)\n",
    "# load the processed xml files\n",
    "loader.load(load_xml=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soups = loader.get_soups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load presidio annotaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soups = load_presidio(soups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.get_chunk_lbl(chunking=True, tokenizer='stanford')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents['actor_10429.xml']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embedding parameters\n",
    "vec_size = 300\n",
    "window = 10\n",
    "sg = 1\n",
    "min_count = 1\n",
    "epochs = 10\n",
    "t = 'word2vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# build the embedding Model\n",
    "embeddingModel = WordEmbedding(sg, vec_size, window, min_count, t=t)\n",
    "# load the model\n",
    "embeddingModel.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or load wiki fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# model_path = \"fasttext_models/wiki.simple.bin\"\n",
    "# model_path = \"fasttext_models/cc.en.300.bin\"\n",
    "# model_path = \"fasttext_models/wiki.en.bin\"\n",
    "\n",
    "# embeddingModel.load(model_path, model_type='fasttext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or load google news vecs in gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = 'word2vec_models/GoogleNews-vectors-negative300.bin.gz'\n",
    "# embeddingModel.load(model_path, model_type='word2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or load bert pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# embeddingModel.model = MyBertEmbedding(size='base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "evaluator1 = Evaluator(loader, embeddingModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the ducuments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gen_soups = evaluator1.export(documents, soups, threshold=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def export_table(evaluator, soups, \n",
    "                 rows=['nertext3', 'nertext4', 'nertext7', 'presidio', 'word2vec'], \n",
    "                 alies=['NER 3', 'NER 4', 'NER 7', 'Presidio', 'Our method'], t=0.15, tp=0, table=None, df=None):\n",
    "    data = {}\n",
    "    if table is None:\n",
    "        table = \"  & Precision & Recall & F1 \\\\\\\\ \\hline\"\n",
    "    for i in range(len(rows)):\n",
    "        if tp == 0:\n",
    "            p, r, f1 = evaluator.evaluate_all(documents, soups, threshold=t, tag=rows[i], silent=True)\n",
    "        elif tp == 1:\n",
    "            p, r, f1 = evaluator.coefficient_of_variation(documents, soups, threshold=t, tag=rows[i])\n",
    "        else:\n",
    "            actor_id = 'actor_19602.xml'\n",
    "            p, r, f1 = evaluator1.evaluate(actor_id, documents, soups, threshold=t, tag=rows[i])\n",
    "        p, r, f1 = p * 100, r * 100, f1 * 100\n",
    "        data[alies[i]] = {w:v for w, v in zip(\"Precision&Recall&F1\".split('&'), [p, r, f1])}\n",
    "        if rows[i] != 'word2vec':\n",
    "            table += \"\\n%s & %.2f\\\\%%  & %.2f\\\\%%  & %.2f\\\\%% \\\\\\\\ \\hline \" % (alies[i], p, r, f1)\n",
    "        else:\n",
    "            table += \"\\n%s & \\\\textbf{%.2f\\\\%%}  & \\\\textbf{%.2f\\\\%%}  & \\\\textbf{%.2f\\\\%%} \\\\\\\\ \\hline \" % (alies[i], p, r, f1)\n",
    "        \n",
    "    if not df is None:\n",
    "        if tp == 0:\n",
    "            df = pd.DataFrame(data).transpose()[\"Precision&Recall&F1\".split('&')]\n",
    "        else:\n",
    "            df = pd.DataFrame(data).transpose()[\"Precision&Recall&F1\".split('&')].\\\n",
    "            rename(columns=dict(zip(\"Precision&Recall&F1\".split('&'), \"Precision CP&Recall CP&F1 CP\".split('&'))))\n",
    "    display(df)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = export_table(evaluator1, gen_soups, t=0.25, tp=0, df=pd.DataFrame())\n",
    "# print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = export_table(evaluator1, gen_soups, t=0.25, tp=1, table=\"  & Precision CP & Recall CP & F1 CP \\\\\\\\ \\hline\", df=1)\n",
    "# print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the anonymized documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_soups = evaluator1.export(documents, soups, threshold=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
