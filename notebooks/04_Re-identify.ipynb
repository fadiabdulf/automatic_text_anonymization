{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "sys.path.insert(1, '..')\n",
    "from classes import DataLoader, MyIterator, Chunker, WordEmbedding, Evaluator, MyBertEmbedding\n",
    "from classes.utils import load_presidio, load_original_article_from_wikipedia, pre_processing, add_annotaion_tag, nlp1\n",
    "from tqdm.auto import tqdm, trange\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set some environment variables to make tensorflow behavior deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "SEED = 2020\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "# Now build your graph and train it\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = '../data/wiki/'\n",
    "out_dir = '../data/wiki/'\n",
    "loader = DataLoader(in_dir, out_dir)\n",
    "# load the processed xml files\n",
    "loader.load(load_xml=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soups = loader.get_soups()\n",
    "soups = OrderedDict(soups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load presidio annotaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soups = load_presidio(soups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.get_chunk_lbl(chunking=True, refresh=True, tokenizer='spacy', testTokenizer=True, originalChunk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = loader.get_lines(abs_tag='originalabstract', chunk=True, tokenizer='stanford', testTokenizer=True, originalChunk=True)\n",
    "docs = list(lines.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and finetune our pre-trained model on the new articles which they are belong to the generalized entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embedding parameters\n",
    "vec_size = 300\n",
    "window = 10\n",
    "sg = 1\n",
    "min_count = 1\n",
    "epochs = 10\n",
    "t = 'fasttext'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# build the embedding Model\n",
    "embeddingModel = WordEmbedding(sg, vec_size, window, min_count, workers=10, t=t)\n",
    "# load the processed xml files\n",
    "in_dir =  '../data/NewArticles/'\n",
    "loader1 = DataLoader(in_dir, in_dir)\n",
    "loader1.load(load_xml=None, generate=True)\n",
    "lines1 = loader1.get_lines(abs_tag='originalabstract', chunk=True, refresh=True, originalChunk=True)\n",
    "docs1 = list(lines1.values())\n",
    "\n",
    "# train the embedding Model\n",
    "embeddingModel.fit(MyIterator(docs + docs1), epochs)\n",
    "# load the model\n",
    "# embeddingModel.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or load wiki fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model_path = \"fasttext_models/wiki.simple.bin\"\n",
    "# model_path = \"fasttext_models/wiki.en.bin\"\n",
    "\n",
    "# embeddingModel.load(model_path, model_type='fasttext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download from wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_df = load_original_article_from_wikipedia(soups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of sentences per article\n",
    "# len(pages_df[pages_df['label'] != 'OTHER']['key'].value_counts()), pages_df[pages_df['label'] != 'OTHER']['key'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, validation splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, dev_df = train_test_split(pages_df, stratify=pages_df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df['text'].apply(lambda x: len(x.split(' ')) >= 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels, dev_texts, dev_labels, unique_labels = pre_processing(train_df, dev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build bert classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, BertConfig, DistilBertConfig, \\\n",
    "                         TFBertModel, BertTokenizerFast, DistilBertTokenizer, BatchEncoding, \\\n",
    "                         TFBertForSequenceClassification, TFDistilBertModel\n",
    "from tokenizers import Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert\n",
    "MODEL_NAME = 'bert-base-cased'\n",
    "# MODEL_NAME = 'distilbert-base-cased'\n",
    "if 'large' in MODEL_NAME:\n",
    "    vector_size = 1024\n",
    "else:\n",
    "    vector_size = 768\n",
    "\n",
    "if 'distil' in MODEL_NAME:\n",
    "    config  = DistilBertConfig.from_pretrained(MODEL_NAME, output_hidden_states=False, num_labels=len(unique_labels))\n",
    "    bert_model = TFDistilBertModel.from_pretrained(MODEL_NAME, config=config)\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "else:\n",
    "    config = BertConfig.from_pretrained(MODEL_NAME, output_hidden_states=False, num_labels=len(unique_labels))\n",
    "    bert_model = TFBertForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "    bert_model.layers[-1].activation = tf.keras.activations.softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "bert_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer([sent for sent in train_texts], padding=True, truncation=True, max_length=512, return_tensors='tf')\n",
    "dev_encodings = tokenizer([sent for sent in dev_texts], padding=True, truncation=True, max_length=512, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(pages_df['label'])\n",
    "y_train = le.transform(train_df['label'])\n",
    "y_train = to_categorical(y_train.reshape(-1, 1))\n",
    "y_dev = le.transform(dev_df['label'])\n",
    "y_dev = to_categorical(y_dev.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_train\n",
    "))\n",
    "dev_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(dev_encodings),\n",
    "    y_dev\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsize = 18\n",
    "history = bert_model.fit(train_dataset.batch(bsize), \n",
    "               epochs=20, \n",
    "               batch_size=bsize,\n",
    "               # validation_split=0.2,\n",
    "               validation_data=dev_dataset.batch(bsize),\n",
    "               callbacks=[EarlyStopping(monitor='val_accuracy', patience=5, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the generalized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "evaluator1 = Evaluator(loader, embeddingModel)\n",
    "gen_soups = evaluator1.export_generalized(documents, soups, threshold=0.25)\n",
    "print(len(gen_soups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add annotation tag\n",
    "gen_soups = add_annotaion_tag(gen_soups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use BERT to predict the names of the actors that Wikipedia article summaries belong to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "from collections import Counter, OrderedDict\n",
    "def my_split2(soup, tag='nertext3'):\n",
    "    orig_text = []\n",
    "    gen_text = []\n",
    "    for e in soup.find(tag).text.split(' '):\n",
    "        if '{' in e and '}' in e and '[' in e and ']' in e:\n",
    "            orig = e[e.index('[')+1:e.index(']')]\n",
    "            gen = e[1:e.index('[')].replace(' ', '_')\n",
    "            origs = orig.split('_')\n",
    "            gens = gen.split('_')\n",
    "            orig_text.extend(origs)\n",
    "#             gen_text.extend(gens)\n",
    "#             orig_text.append(origs[0])\n",
    "            gen_text.append(gens[0])\n",
    "#             gen_text.append(gen)\n",
    "        else:\n",
    "            orig_text.extend(e.split('_'))\n",
    "            gen_text.extend(e.split('_'))\n",
    "    # print(' '.join(gen_text))\n",
    "    return orig_text, gen_text\n",
    "\n",
    "def predict(soup, tag='nertext3'):\n",
    "    orig_text, gen_text = my_split2(soup, tag=tag)\n",
    "    # print(gen_text)\n",
    "    orig_sents = [str(sent) for sent in nlp1(' '.join(orig_text)).sents]\n",
    "    gen_sents = []\n",
    "    offset = 0\n",
    "    for sent in orig_sents:\n",
    "        gen_sents.append(' '.join(gen_text[offset:offset + sent.count(' ') + 1]))\n",
    "        offset += sent.count(' ') + 1\n",
    "    # gen_sents = [str(sent) for sent in nlp1(gen_text).sents]\n",
    "    orig_tokens = tokenizer(orig_sents, padding=True, truncation=True, max_length=512, return_tensors='tf')\n",
    "    gen_tokens = tokenizer(gen_sents, padding=True, truncation=True, max_length=512, return_tensors='tf')\n",
    "    orig_out = bert_model(orig_tokens)[0].numpy().argmax(axis=1)\n",
    "    gen_out = bert_model(gen_tokens)[0].numpy().argmax(axis=1)\n",
    "    return list(le.inverse_transform(orig_out)), list(le.inverse_transform(gen_out))\n",
    "    \n",
    "def evaluate(soup, tag='nertext3'):\n",
    "    original = False\n",
    "    if tag == 'original':\n",
    "        tag = 'nertext3'\n",
    "        original = True\n",
    "    label = soup.find('title').text\n",
    "    orig_labels, gen_labels = predict(soup, tag=tag)\n",
    "    if original:\n",
    "        vc = Counter(orig_labels)\n",
    "    else:\n",
    "        vc = Counter(gen_labels)\n",
    "    vc = OrderedDict(sorted(vc.items(), key=lambda kv: kv[1], reverse=True))\n",
    "    # print(vc)\n",
    "    # print(vc.items())\n",
    "    pred = list(vc)[0]\n",
    "    if len(vc) > 1:\n",
    "        if vc[list(vc)[0]] == vc[list(vc)[1]]:\n",
    "            pred = 'OTHER'\n",
    "    return label, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate BERT prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['original', 'annotation', 'nertext3', 'nertext4', 'nertext7', 'presidio', 'word2vec', 'word2vec_gen']\n",
    "aliases = ['Original summary', 'Manual annotation', 'NER 3', 'NER 4', 'NER 7', 'Presidio', 'Our method', 'Our method + gen']\n",
    "data = []\n",
    "cols = ['tag', 'file', 'orig', 'pred']\n",
    "\n",
    "for i, tag in tqdm(enumerate(tags), total=len(tags)):\n",
    "    alias = aliases[i]\n",
    "    count = 0\n",
    "    for key in gen_soups:\n",
    "        if gen_soups[key].find('title').text in unique_labels:\n",
    "            orig, pred = evaluate(gen_soups[key], tag=tag)\n",
    "            data.append([alias, key, orig, pred])\n",
    "    # data.append([tag, 'general', 'OTHER', 'OTHER'])\n",
    "df1 = pd.DataFrame(data, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary level\n",
    "columns = ['Input', 'Correct predictions', 'Correct predictions %']\n",
    "data = []\n",
    "for tag, group in tqdm(df1.groupby('tag', sort=False), total=len(df1['tag'].value_counts())):\n",
    "    count = sum([x == y for x, y in zip(group['orig'], group['pred'])])\n",
    "    data.append([tag, count, count / len(gen_soups)])\n",
    "df3 = pd.DataFrame(data, columns=columns)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
